# Last Eat — Improvement Roadmap

> Each phase is designed to be completable in a single Claude session.
> Status: `[ ]` pending · `[~]` in progress · `[x]` done

---

## Phase 1: Foundation — CI/CD & Scraper Reliability
_Goal: Automate data freshness and prevent silent breakage._

### 1A. Automated Scraper Pipeline `[x]`
**Why:** Data goes stale because scraper + data.js update is manual.
**Scope:**
- Create `.github/workflows/scrape.yml` — weekly cron (Sunday 03:00 UTC)
- Workflow runs `scraper.py --fresh --enrich`, transforms output to `docs/data.js`
- Auto-commits and pushes if data changed (skip if identical)
- Add a `scripts/generate_data_js.py` that reads `output/madrid_restaurants.json` and writes `docs/data.js` in the abbreviated key format
- Update footer timestamp automatically from pipeline

**Files to create:**
- `.github/workflows/scrape.yml`
- `scripts/generate_data_js.py`

**Files to modify:**
- `scraper.py` — make it exit with error code on validation failure
- `docs/index.html` — footer reads timestamp from data.js instead of hardcoded

**Acceptance criteria:**
- [x] Workflow runs successfully in GitHub Actions
- [x] `docs/data.js` is generated programmatically (not hand-edited)
- [x] Footer shows dynamic "Actualizado" date
- [x] Pipeline skips commit when data is unchanged

---

### 1B. Scraper Hardening `[x]`
**Why:** If Macarfi changes HTML structure, scraper silently returns empty fields.
**Scope:**
- Pin dependency versions in `requirements.txt`
- Add validation after scraping: assert minimum thresholds (e.g. ≥700 restaurants, ≥90% with cuisine data)
- Add retry with backoff for transient HTTP errors in `enrich_from_detail`
- Log warnings for restaurants missing expected fields
- If validation fails in CI, open a GitHub Issue automatically

**Files to modify:**
- `requirements.txt` — pin versions
- `scraper.py` — add validation function, retry logic, structured logging
- `.github/workflows/scrape.yml` — add failure notification step

**Acceptance criteria:**
- [x] `requirements.txt` has pinned versions
- [x] Scraper exits non-zero if data quality drops below thresholds
- [x] HTTP retries work for transient 5xx errors
- [x] CI creates a GitHub Issue on scraper failure

---

## Phase 2: SEO & Shareability
_Goal: Make restaurants discoverable via search engines and shareable on social._

### 2A. Individual Restaurant Pages `[ ]`
**Why:** Can't link to or share a specific restaurant. Zero SEO surface.
**Scope:**
- Create `scripts/generate_pages.py` — generates `docs/r/{slug}.html` per restaurant
- Each page has: name, rating, cuisine, district, price, address, map embed, link back to main site
- Proper `<title>`, `<meta description>`, Open Graph tags, `schema.org/Restaurant` structured data
- Canonical URL: `https://lasteat.es/r/{slug}.html`
- Add link from main site cards to individual pages

**Files to create:**
- `scripts/generate_pages.py`
- `docs/r/.gitkeep` (directory placeholder; generated pages will be many)

**Files to modify:**
- `docs/index.html` — card links point to `/r/{slug}.html` instead of Macarfi
- `.github/workflows/scrape.yml` — add page generation step
- `.gitignore` — consider whether generated pages are committed or built in CI

**Acceptance criteria:**
- [ ] Each restaurant has a `/r/{slug}.html` page
- [ ] Pages have correct OG tags (test with https://opengraph.dev)
- [ ] Pages have schema.org/Restaurant JSON-LD
- [ ] Google can crawl individual pages (add `docs/sitemap.xml`)

---

### 2B. Sitemap & SEO Metadata `[ ]`
**Why:** Search engines need a sitemap to discover all restaurant pages.
**Scope:**
- Generate `docs/sitemap.xml` listing all restaurant pages + homepage
- Add `docs/robots.txt` allowing all crawlers
- Add `<meta>` tags to `index.html` (description, OG image, twitter card)
- Create a simple OG image (static, branded — `docs/og.png`)

**Files to create:**
- `docs/sitemap.xml` (generated by script)
- `docs/robots.txt`
- `docs/og.png` (static branded image)

**Files to modify:**
- `scripts/generate_pages.py` — also generates sitemap.xml
- `docs/index.html` — add meta tags in `<head>`

**Acceptance criteria:**
- [ ] `sitemap.xml` lists all URLs
- [ ] `robots.txt` exists and allows crawling
- [ ] Homepage has OG tags with image
- [ ] Google Search Console can read the sitemap (manual verification)

---

## Phase 3: User Experience Enhancements
_Goal: Turn the site into a genuinely useful tool for people in Madrid._

### 3A. "Near Me" Geolocation `[ ]`
**Why:** All 770 restaurants have coordinates but no proximity feature exists.
**Scope:**
- Add "Cerca de mi" button in controls bar
- Request browser geolocation on click
- Calculate haversine distance from user to each restaurant
- Show distance on cards (e.g. "1.2 km")
- Add distance as a sort option
- On map view, show user position marker + optional radius circle

**Files to modify:**
- `docs/index.html` — new button, distance calc, sort option, card display, map marker

**Acceptance criteria:**
- [ ] Button requests geolocation permission
- [ ] Cards show distance when location is active
- [ ] Distance sort works correctly
- [ ] Map shows user position
- [ ] Graceful fallback if permission denied

---

### 3B. Progressive Web App `[ ]`
**Why:** Madrid visitors have spotty data. Offline access makes this a pocket guide.
**Scope:**
- Create `docs/manifest.json` with app name, icons, theme color
- Create `docs/sw.js` service worker — cache-first for data.js, network-first for HTML
- Generate app icons (multiple sizes) from a base icon
- Add install prompt / "Add to Home Screen" hint
- Cache favorited restaurant pages for offline priority

**Files to create:**
- `docs/manifest.json`
- `docs/sw.js`
- `docs/icons/` (icon-192.png, icon-512.png minimum)

**Files to modify:**
- `docs/index.html` — register service worker, link manifest, add apple-touch-icon

**Acceptance criteria:**
- [ ] Lighthouse PWA audit passes
- [ ] App installable on mobile
- [ ] Site works offline after first visit
- [ ] Cached data updates on next online visit

---

## Phase 4: Discovery & Scale
_Goal: Better ways to find restaurants and expand to more cities._

### 4A. Smart Search & Discovery `[ ]`
**Why:** Current search is exact substring match — typos fail, no serendipity.
**Scope:**
- Add fuzzy matching (lightweight, no library — trigram or Levenshtein on client)
- "Surprise me" button: random pick from current filtered set, with animation
- Quick-filter tags above the grid: "Top 50", "Cheap eats (<30€)", "Best service"
- Result count badge on active filters for discoverability

**Files to modify:**
- `docs/index.html` — fuzzy search logic, surprise button, quick-filter tags

**Acceptance criteria:**
- [ ] "divesro" matches "Diverxo"
- [ ] Surprise button picks a random restaurant and scrolls to it
- [ ] Quick-filter tags filter correctly and compose with other filters
- [ ] Performance remains smooth with 770 restaurants

---

### 4B. Multi-City Architecture `[ ]`
**Why:** Macarfi covers Barcelona, Valencia, Sevilla. Same scraper can serve them all.
**Scope:**
- Parameterize scraper: `python scraper.py --city mad|bcn|vlc`
- City config dict: API location IDs, names, map center coordinates
- Generate per-city data files: `docs/data-mad.js`, `docs/data-bcn.js`
- Add city switcher dropdown to frontend
- Update CI workflow to scrape all configured cities

**Files to modify:**
- `scraper.py` — city parameter, config dict
- `scripts/generate_data_js.py` — per-city output
- `docs/index.html` — city switcher, dynamic data loading
- `.github/workflows/scrape.yml` — loop over cities

**Acceptance criteria:**
- [ ] Scraper works for at least 2 cities
- [ ] Frontend loads correct data per city selection
- [ ] URL state includes city parameter
- [ ] CI scrapes all cities

---

## Phase 5: Polish & Insights
_Goal: Optimize performance and understand user behavior._

### 5A. Performance Optimization `[ ]`
**Why:** Reduce load time, especially on mobile networks.
**Scope:**
- Self-host fonts (download woff2, inline critical CSS)
- Minify inline CSS/JS in index.html (or add a tiny build step)
- Add `<link rel="preconnect">` for CDN resources
- Compress data.js with more aggressive abbreviation or split lazy-load
- Add `<meta>` viewport hints, preload critical resources

**Files to modify:**
- `docs/index.html` — font loading, preconnects, minification
- Possibly create `docs/fonts/` for self-hosted font files

**Acceptance criteria:**
- [ ] Lighthouse Performance score ≥95
- [ ] No render-blocking external requests
- [ ] First Contentful Paint <1.5s on 3G throttle

---

### 5B. Privacy-Friendly Analytics `[ ]`
**Why:** No visibility into usage. Need data to prioritize future work.
**Scope:**
- Add Plausible or Umami script tag (lightweight, cookie-free)
- Track: page views, search queries (aggregated), filter usage, map vs grid
- Custom events: favorite toggles, card expansions, outbound clicks
- Dashboard accessible to project owner

**Files to modify:**
- `docs/index.html` — analytics script tag, custom event calls

**Acceptance criteria:**
- [ ] Analytics dashboard shows page views
- [ ] No cookies set, GDPR-compliant
- [ ] Custom events fire correctly
- [ ] <1KB script overhead

---

## Session Hand-off Protocol

When starting a new session on this project:
1. Read `CLAUDE.md` for project conventions
2. Read `ROADMAP.md` (this file) to find the next incomplete phase
3. Check `git log --oneline -10` for recent changes
4. Check `git status` for any uncommitted work
5. Work on the next `[ ]` item within the current incomplete phase
6. After completing a task, update its checkboxes to `[x]` in this file
7. Commit the roadmap update along with the implementation
