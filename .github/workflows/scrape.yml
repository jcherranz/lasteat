name: Update restaurant data

on:
  schedule:
    # Every Sunday at 03:00 UTC
    - cron: '0 3 * * 0'
  workflow_dispatch: # Allow manual trigger

permissions:
  contents: write
  pull-requests: write
  issues: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2

      - name: Set up Python
        uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # v6.2.0
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run tests
        run: pytest

      - name: Snapshot previous quality and data baselines
        run: |
          [ -f docs/quality_report.json ] && cp docs/quality_report.json /tmp/quality_report_prev.json || echo "No previous quality report"
          [ -f docs/data.js ] && cp docs/data.js /tmp/data_prev.js || echo "No previous data.js"

      - name: Run scraper
        id: scrape
        run: python scraper.py --fresh --enrich

      - name: Publish quality report snapshot
        run: cp output/quality_report.json docs/quality_report.json

      - name: Warn on quality coverage drops >5%
        run: |
          python - <<'PY'
          import json
          from pathlib import Path

          old_path = Path("/tmp/quality_report_prev.json")
          new_path = Path("output/quality_report.json")
          if not old_path.exists() or not new_path.exists():
              print("No previous/new quality report available; skipping quality trend warning.")
              raise SystemExit(0)

          old = json.loads(old_path.read_text(encoding="utf-8"))
          new = json.loads(new_path.read_text(encoding="utf-8"))
          old_fields = old.get("fields", {})
          new_fields = new.get("fields", {})

          for field_name, new_stats in sorted(new_fields.items()):
              if field_name not in old_fields:
                  continue
              old_pct = float(old_fields[field_name].get("pct", 0.0))
              new_pct = float(new_stats.get("pct", 0.0))
              drop = old_pct - new_pct
              if drop > 0.05:
                  print(
                      f"::warning::Coverage drop >5% for {field_name}: "
                      f"{old_pct*100:.1f}% -> {new_pct*100:.1f}% "
                      f"(drop {drop*100:.1f}pp)"
                  )
          PY

      - name: Generate data.js
        run: python scripts/generate_data_js.py

      - name: Compare data diff and fail on >50% removals
        run: |
          if [ -f /tmp/data_prev.js ]; then
            python scripts/compare_data.py /tmp/data_prev.js docs/data.js --max-removed-pct 50 --json-out output/data_diff_report.json
          else
            echo "No previous data.js snapshot; skipping diff guard."
          fi

      - name: Warn on data.js size anomaly >20%
        run: |
          if [ -f /tmp/data_prev.js ]; then
            python - <<'PY'
            from pathlib import Path

            old_path = Path("/tmp/data_prev.js")
            new_path = Path("docs/data.js")
            old_size = old_path.stat().st_size
            new_size = new_path.stat().st_size
            if old_size == 0:
                print("Previous data.js size is zero; skipping size anomaly warning.")
                raise SystemExit(0)

            change_pct = ((new_size - old_size) / old_size) * 100
            if abs(change_pct) > 20:
                direction = "grew" if change_pct > 0 else "shrank"
                print(
                    f"::warning::data.js {direction} by {abs(change_pct):.1f}% "
                    f"({old_size} -> {new_size} bytes)"
                )
            else:
                print(
                    f"data.js size change within threshold: "
                    f"{change_pct:.1f}% ({old_size} -> {new_size} bytes)"
                )
            PY
          else
            echo "No previous data.js snapshot; skipping size warning."
          fi

      - name: Generate restaurant pages and sitemap
        run: python scripts/generate_pages.py

      - name: Check for changes
        id: diff
        run: |
          git add docs/data.js docs/quality_report.json docs/r/ docs/sitemap.xml
          git diff --cached --quiet && echo "changed=false" >> "$GITHUB_OUTPUT" || echo "changed=true" >> "$GITHUB_OUTPUT"

      - name: Commit and push update branch
        if: steps.diff.outputs.changed == 'true'
        env:
          UPDATE_BRANCH: automation/data-update
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git commit -m "Update restaurant data $(date -u +%Y-%m-%d)"
          git push --force origin HEAD:$UPDATE_BRANCH

      - name: Create or update pull request
        if: steps.diff.outputs.changed == 'true'
        env:
          GH_TOKEN: ${{ github.token }}
          UPDATE_BRANCH: automation/data-update
        run: |
          PR_TITLE="Update restaurant data $(date -u +%Y-%m-%d)"
          PR_BODY=$(
            cat <<'BODY'
          Automated restaurant data update.

          Includes:
          - refreshed scraped dataset
          - regenerated `docs/data.js`
          - regenerated restaurant pages and sitemap
          - refreshed quality report snapshot (`docs/quality_report.json`)
          BODY
          )

          PR_NUMBER=$(gh pr list --head "$UPDATE_BRANCH" --base main --state open --json number --jq '.[0].number // empty')
          if [ -n "$PR_NUMBER" ]; then
            gh pr edit "$PR_NUMBER" --title "$PR_TITLE" --body "$PR_BODY"
            echo "Updated PR #$PR_NUMBER"
          else
            gh pr create --base main --head "$UPDATE_BRANCH" --title "$PR_TITLE" --body "$PR_BODY"
            echo "Created new PR for $UPDATE_BRANCH"
          fi

      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            const title = `Scraper failed â€” ${new Date().toISOString().slice(0, 10)}`;
            const body = [
              'The automated restaurant data update failed.',
              '',
              `**Run:** ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              '',
              'Possible causes:',
              '- Macarfi website changed its HTML structure',
              '- API endpoint returned an error',
              '- Data validation thresholds not met',
            ].join('\n');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['bug', 'automated'],
            });
